{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "#import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "#from tensorflow.keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.layers import Embedding, Concatenate, Add, Activation,Dot\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten,Reshape, MaxPooling2D,MaxPooling3D\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling1D, Embedding, Dropout, AdditiveAttention, Multiply\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "user_vocab_size = 1000      \n",
    "item_vocab_size = 500\n",
    "user_review_len = 200\n",
    "item_review_len = 200\n",
    "user_review_num = 2000\n",
    "item_review_num = 2000\n",
    "\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 20\n",
    "conv_filters = 128\n",
    "drop_rate = 0.2\n",
    "attention_units = 32\n",
    "embedding_id = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_word_index = {\"hello\":0,\"world\":1}\n",
    "item_word_index = {\"bye\":0,\"world\":1}\n",
    "USER_SEQUENCE_LENGTH = 20\n",
    "input_u =[\"not good\",\"busy pretty\",\"not tidy\",\"good service\",\"good food\",\"cheap place\",\"parking is cheap\",\"location good\",\"good ambience\",\"friends party\"]\n",
    "input_i =[\"not good\",\"not tidy\",\"good service\",\"good food\",\"cheap place\",\"friends party\",\"not hygenic\",\"people are helpful\",\"spot\"]\n",
    "\n",
    "\n",
    "ITEM_SEQUENCE_LENGTH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Total 400000 word vectors in Glove 6B 100d.\n"
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt',encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors in Glove 6B 100d.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embedding_matrix = np.random.random((len(user_word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in user_word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        user_embedding_matrix[i] = embedding_vector\n",
    "user_embedding_layer = Embedding(len(user_word_index) + 1,\n",
    "                            EMBEDDING_DIM,weights=[user_embedding_matrix],\n",
    "                            input_length=USER_SEQUENCE_LENGTH,trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_embedding_matrix = np.random.random((len(item_word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in item_word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        item_embedding_matrix[i] = embedding_vector\n",
    "item_embedding_layer = Embedding(len(item_word_index) + 1,\n",
    "                            EMBEDDING_DIM,weights=[item_embedding_matrix],\n",
    "                            input_length=ITEM_SEQUENCE_LENGTH,trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(None, 20, 2000)\n(None, 20, 2000, 100)\n(None, 2000, 20, 100, 1)\n(None, 2000, 18, 1, 128)\n(None, 2000, 1, 1, 128)\n(None, 256000)\n(None, 2000, 17, 1, 128)\n(None, 2000, 1, 1, 128)\n(None, 256000)\n(None, 2000, 16, 1, 128)\n(None, 2000, 1, 1, 128)\n(None, 256000)\n(None, 768000)\n"
    }
   ],
   "source": [
    "user_sequence_input = Input(shape=(USER_SEQUENCE_LENGTH,user_review_num), dtype='int32')\n",
    "print(user_sequence_input.shape)\n",
    "user_embedded_reviews = user_embedding_layer(user_sequence_input)\n",
    "print(user_embedded_reviews.shape)\n",
    "user_embedded_reviews_flat = Reshape((user_review_num,USER_SEQUENCE_LENGTH,EMBEDDING_DIM,1))(user_embedded_reviews)\n",
    "print(user_embedded_reviews_flat.shape)\n",
    "conv_out = []\n",
    "for f_size in filter_sizes:\n",
    "    l_cov1= Conv3D(conv_filters, (1,f_size,EMBEDDING_DIM), activation='relu',padding='valid')(user_embedded_reviews_flat)\n",
    "    print(l_cov1.shape)\n",
    "    l_pool1 = MaxPooling3D(pool_size =(1,USER_SEQUENCE_LENGTH-f_size+1,1),padding='valid')(l_cov1)\n",
    "    print(l_pool1.shape)\n",
    "    l_flat = Flatten()(l_pool1)\n",
    "    print(l_flat.shape)\n",
    "    conv_out.append(l_flat)\n",
    "conv_joined = Concatenate()(conv_out)\n",
    "print(conv_joined.shape)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(None, 2000, 20, 100, 1)\n(None, 2000, 18, 1, 128)\n(None, 2000, 1, 1, 128)\n(None, 256000)\n(None, 2000, 17, 1, 128)\n(None, 2000, 1, 1, 128)\n(None, 256000)\n(None, 2000, 16, 1, 128)\n(None, 2000, 1, 1, 128)\n(None, 256000)\n(None, 768000)\n"
    }
   ],
   "source": [
    "item_sequence_input = Input(shape=(ITEM_SEQUENCE_LENGTH,item_review_num), dtype='int32')\n",
    "item_embedded_reviews = item_embedding_layer(item_sequence_input)\n",
    "item_embedded_reviews_flat = Reshape((item_review_num,ITEM_SEQUENCE_LENGTH,EMBEDDING_DIM,1))(item_embedded_reviews)\n",
    "print(item_embedded_reviews_flat.shape)\n",
    "item_conv_out = []\n",
    "for f_size in filter_sizes:\n",
    "    l_cov1= Conv3D(conv_filters, (1,f_size,EMBEDDING_DIM), activation='relu',padding='valid')(item_embedded_reviews_flat)\n",
    "    print(l_cov1.shape)\n",
    "    l_pool1 = MaxPooling3D(pool_size =(1,ITEM_SEQUENCE_LENGTH-f_size+1,1),padding='valid')(l_cov1)\n",
    "    print(l_pool1.shape)\n",
    "    l_flat = Flatten()(l_pool1)\n",
    "    print(l_flat.shape)\n",
    "    item_conv_out.append(l_flat)\n",
    "item_conv_joined = Concatenate()(item_conv_out)\n",
    "print(item_conv_joined.shape)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(None, 2000, 384)\n(None, 2000, 32)\n(None, 2000, 32)\n(None, 2000, 32)\n(None, 2000, 32)\n(None, 2000, 1)\n(None, 2000, 384)\n(None, 384)\n"
    }
   ],
   "source": [
    "\n",
    "user_flat = Reshape((user_review_num,conv_filters*len(filter_sizes)))(conv_joined)\n",
    "print(user_flat.shape)\n",
    "#user_drop = Dropout(1.0)(user_flat)\n",
    "\n",
    "total_item = 1000000\n",
    "u_iid = Input(shape=(user_review_num), dtype='int32')\n",
    "item_id_embedding = Embedding(total_item + 2,\n",
    "                            embedding_id,\n",
    "                            input_length=1,trainable=True)\n",
    "item_embs = item_id_embedding(u_iid)\n",
    "item_embs = Activation('relu')(item_embs)\n",
    "print(item_embs.shape)\n",
    "##\n",
    "\n",
    "user_atten = Dense(attention_units,kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001))(user_flat)\n",
    "print(user_atten.shape) \n",
    "item_id_atten = Dense(attention_units,kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001))(item_embs)\n",
    "print(item_id_atten.shape)\n",
    "added = Add()([user_atten,item_id_atten])\n",
    "print(added.shape)\n",
    "added = Activation('relu')(added)\n",
    "user_a = Dense(1)(added)\n",
    "print(user_a.shape)\n",
    "user_a = tf.keras.activations.softmax(user_a)\n",
    "#user_a= AdditiveAttention()([user_flat,item_embs])\n",
    "u_feas = Multiply()([user_flat,user_a])\n",
    "print(u_feas.shape)\n",
    "u_feas  = tf.keras.backend.sum(u_feas,axis = 1)\n",
    "print(u_feas.shape)\n",
    "u_feas = Dropout(drop_rate)(u_feas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(None, 2000, 32)\n(None, 2000, 32)\n(None, 2000, 32)\n(None, 2000, 1)\n(None, 2000, 384)\n(None, 384)\n"
    }
   ],
   "source": [
    "item_flat = Reshape((item_review_num,conv_filters*len(filter_sizes)))(item_conv_joined)\n",
    "#item_drop = Dropout(1.0)(item_flat)\n",
    "total_users = 200000\n",
    "i_uid = Input(shape=(item_review_num,), dtype='int32')\n",
    "user_id_embedding = Embedding(total_users + 2,\n",
    "                            embedding_id,\n",
    "                            input_length=1,trainable=True)\n",
    "user_embs = user_id_embedding(i_uid)\n",
    "user_embs = Activation('relu')(user_embs)\n",
    "\n",
    "item_atten = Dense(attention_units,kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001))(item_flat)\n",
    "print(item_atten.shape) \n",
    "user_id_atten = Dense(attention_units,kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001))(user_embs)\n",
    "print(user_id_atten.shape)\n",
    "item_added = Add()([item_atten,user_id_atten])\n",
    "print(item_added.shape)\n",
    "item_added = Activation('relu')(item_added)\n",
    "item_a = Dense(1)(item_added)\n",
    "print(item_a.shape)\n",
    "item_a = tf.keras.activations.softmax(item_a,axis=1)\n",
    "#user_a= AdditiveAttention()([user_flat,item_embs])\n",
    "i_feas = Multiply()([item_flat,item_a])\n",
    "print(i_feas.shape)\n",
    "i_feas  = tf.keras.backend.sum(i_feas,axis = 1)\n",
    "print(i_feas.shape)\n",
    "i_feas = Dropout(drop_rate)(i_feas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(None, 1, 32)\n(None, 1, 32)\n(None, 2000, 32)\n(None, 1, 32) (None, 1, 32)\n(None, 1, 1)\n"
    }
   ],
   "source": [
    "uid = Input(shape=(1), dtype='int32')\n",
    "iid = Input(shape=(1,), dtype='int32')\n",
    "item_id_embedding = Embedding(total_item + 2,\n",
    "                            embedding_id,\n",
    "                            input_length=1,trainable=True)\n",
    "item_id_emb = item_id_embedding(iid)\n",
    "print(item_id_emb.shape)\n",
    "#item_embs = Activation('relu')(item_embs)\n",
    "user_id_embedding = Embedding(total_users + 2,\n",
    "                            embedding_id,\n",
    "                            input_length=1,trainable=True)\n",
    "user_id_emb = user_id_embedding(uid)\n",
    "print(user_id_emb.shape)\n",
    "#user_embs = Activation('relu')(user_embs)\n",
    "u_feas_latent = Dense(embedding_id)(u_feas)\n",
    "print(user_id_atten.shape)\n",
    "u_feas = Add()([u_feas_latent,user_id_emb])\n",
    "\n",
    "i_feas_latent = Dense(embedding_id)(i_feas)\n",
    "i_feas = Add()([i_feas_latent,item_id_emb])\n",
    "print(i_feas.shape,u_feas.shape)\n",
    "\n",
    "preds = Dot(axes=-1)([u_feas ,i_feas])\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[], outputs=preds)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])"
   ]
  }
 ]
}